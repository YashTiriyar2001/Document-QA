# -*- coding: utf-8 -*-
"""Document QA updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxA37biuGf56Px6YNkesY2Nljn3bA7eB
"""

from google.colab import drive
drive.mount('/content/drive')

# !pip install 'git+https://github.com/facebookresearch/detectron2.git'
!pip install transformers datasets
!pip install torchvision
!pip install pytesseract
!sudo apt install tesseract-ocr

#importing libraries
from PIL import Image
import pandas as pd
import pytesseract
from transformers import AutoProcessor
from datasets import load_dataset, Dataset, DatasetDict
from transformers import TrainingArguments
from transformers import AutoModelForDocumentQuestionAnswering
from transformers import DefaultDataCollator
from transformers import Trainer
from transformers import AutoTokenizer


class document_question_answering:
    def image_dataset(self,conf, dataset_dict):
      #Calling csv dataset for dataset_dict
      dataset_dict = self.csv_dataset(conf)
      #Importing images from directory which is in .jpeg format and splitting it into train and test value
      dir_path = conf['parent_dir']
      train_image = []

      for i in range(len(dataset_dict['train'])):
        image_name = dataset_dict['train']['image_name'][i]
        image_path = dir_path + "/" + "image" + "/" + f"{image_name}.jpeg"
        image = Image.open(image_path)
        train_image.append(image)

      test_image = []

      for i in range(len(dataset_dict['test'])):
        image_name = dataset_dict['test']['image_name'][i]
        image_path = dir_path + "/" + "image" + "/" + f"{image_name}.jpeg"
        image = Image.open(image_path)
        test_image.append(image)

      return train_image, test_image
  


    def csv_dataset(self, conf):
      #Importing CSV file from the directory
      path = conf["parent_dir"] + "/" + "csv" + "/" + "document.csv"
      df = pd.read_csv(path)

      train_df = df.iloc[:-1]
      test_df = df.iloc[-1:]

      train_dataset = Dataset.from_pandas(train_df)
      test_dataset = Dataset.from_pandas(test_df)
      
      #Creating a dataset from csv file and splitting it into train and test values

      dataset_dict = DatasetDict({"train": train_dataset, "test": test_dataset})
      dataset_dict["train"].set_format(type='pandas', columns=['id', conf["feature_columns"], conf["label_column"], 'image_name'])
      dataset_dict["test"].set_format(type='pandas', columns=['id', conf["feature_columns"], conf["label_column"], 'image_name'])
      return dataset_dict

    def loading_dataset(self,conf):
        #Calling required functions to get the image data and csv data
        dataset_dict = self.csv_dataset(conf)
        train_image, test_image = self.image_dataset(conf, dataset_dict)


        #creating a new dataset which involves image and csv in the structure which is supported by our pre-trained model "microsoft/layoutlmv2-base-uncased"
        new_dataset_train = Dataset.from_dict({
          'id' : dataset_dict['train']['id'],
          conf["feature_columns"] : dataset_dict['train'][conf["feature_columns"]],
          conf["label_column"] : dataset_dict['train'][conf["label_column"]],
          'image_name' : dataset_dict['train']['image_name'],
          'image' : train_image
              
        })

        new_dataset_test = Dataset.from_dict({
          'id' : dataset_dict['test']['id'],
          conf["feature_columns"] : dataset_dict['test'][conf["feature_columns"]],
          conf["label_column"] : dataset_dict['test'][conf["label_column"]],
          'image_name' : dataset_dict['test']['image_name'],
          'image' : test_image
        })

        dataset = DatasetDict({'train': new_dataset_train, 'test': new_dataset_test})
        updated_dataset = dataset.map(lambda example: {conf["feature_columns"]: example[conf["feature_columns"]]}, remove_columns=[conf["feature_columns"]])
        updated_dataset = updated_dataset.map(
          lambda example: {conf["label_column"]: example[conf["label_column"]][0]}, remove_columns=[conf["label_column"]]
        )

        return updated_dataset




    def data_preprocessing(self,conf,updated_dataset):
        
        #Calling image_preprocessing and text_preprocessing for data encoding
        
        processor = AutoProcessor.from_pretrained(conf["model_checkpoint"])
        
        dataset_with_ocr = self.image_preprocessing(updated_dataset,processor)
        encoded_train_dataset, encoded_test_dataset= self.text_preprocessing(dataset_with_ocr,processor)
        return encoded_train_dataset, encoded_test_dataset, processor

        

    def image_preprocessing(self,updated_dataset,processor):
        # Function performs additional preprocessing on the combined dataset, specifically for the image data. It calls image_preprocessing to perform OCR on the images and extract words and bounding boxes, and then calls text_preprocessing to encode the text data using the Hugging Face tokenizer.
        
        # Add image preprocessing code here
        image_processor = processor.image_processor
        def get_ocr_words_and_boxes(examples):
            
            images = [image.convert("RGB") for image in examples["image"]]
            encoded_inputs = image_processor(images)
            examples["image"] = encoded_inputs.pixel_values
            examples["words"] = encoded_inputs.words
            examples["boxes"] = encoded_inputs.boxes
            return examples
        dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
        return dataset_with_ocr
    

    def text_preprocessing(self, dataset_with_ocr,processor):
        # Add text preprocessing code here
        tokenizer = processor.tokenizer

        def subfinder(words_list, answer_list):
            matches = []
            start_indices = []
            end_indices = []
            for idx, i in enumerate(range(len(words_list))):
                if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
                    matches.append(answer_list)
                    start_indices.append(idx)
                    end_indices.append(idx + len(answer_list) - 1)
            if matches:
                return matches[0], start_indices[0], end_indices[0]
            else:
                return None, 0, 0

        example = dataset_with_ocr["train"][1]
        words = [word.lower() for word in example["words"]]
        match, word_idx_start, wordd_idx_end = subfinder(words,example[conf["label_column"]].lower().split())
        encoding = tokenizer(example[conf["feature_columns"]], example["words"], example["boxes"])

        def encode_dataset(examples, max_length=512):
            questions = examples[conf["feature_columns"]]
            words = examples["words"]
            boxes = examples["boxes"]
            answers = examples[conf["label_column"]]

            # encode the batch of examples and initialize the start_positions and end_positions
            encoding = tokenizer(questions, words, boxes, max_length=max_length, padding="max_length", truncation=True)
            start_positions = []
            end_positions = []

            # loop through the examples in the batch
            for i in range(len(questions)):
                cls_index = encoding["input_ids"][i].index(tokenizer.cls_token_id)

                # find the position of the answer in example's words
                words_example = [word.lower() for word in words[i]]
                answer = answers[i]
                match, word_idx_start, word_idx_end = subfinder(words_example, answer.lower().split())

                if match:
                    # if match is found, use `token_type_ids` to find where words start in the encoding
                    token_type_ids = encoding["token_type_ids"][i]
                    token_start_index = 0
                    while token_type_ids[token_start_index] != 1:
                        token_start_index += 1

                    token_end_index = len(encoding["input_ids"][i]) - 1
                    while token_type_ids[token_end_index] != 1:
                        token_end_index -= 1

                    word_ids = encoding.word_ids(i)[token_start_index : token_end_index + 1]
                    start_position = cls_index
                    end_position = cls_index

                    # loop over word_ids and increase `token_start_index` until it matches the answer position in words
                    # once it matches, save the `token_start_index` as the `start_position` of the answer in the encoding
                    for id in word_ids:
                        if id == word_idx_start:
                            start_position = token_start_index
                        else:
                            token_start_index += 1

                    # similarly loop over `word_ids` starting from the end to find the `end_position` of the answer
                    for id in word_ids[::-1]:
                        if id == word_idx_end:
                            end_position = token_end_index
                        else:
                            token_end_index -= 1

                    start_positions.append(start_position)
                    end_positions.append(end_position)

                else:
                    start_positions.append(cls_index)
                    end_positions.append(cls_index)

            encoding["image"] = examples["image"]
            encoding["start_positions"] = start_positions
            encoding["end_positions"] = end_positions

            return encoding

        encoded_train_dataset = dataset_with_ocr["train"].map(
            encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["train"].column_names
        )
        encoded_test_dataset = dataset_with_ocr["test"].map(
            encode_dataset, batched=True, batch_size=2, remove_columns=dataset_with_ocr["test"].column_names
        )
        return encoded_train_dataset, encoded_test_dataset





    def training(self,conf,encoded_train_dataset, encoded_test_dataset,processor):
  
        #Model Training from pretrained model and the encoded dataset
        model = AutoModelForDocumentQuestionAnswering.from_pretrained(conf["model_checkpoint"])



        training_args = TrainingArguments(
            output_dir= conf['output_dir'],
            per_device_train_batch_size= conf['batch_size'],
            num_train_epochs= conf['epoch'],
            save_steps=200,
            logging_steps=50,
            evaluation_strategy="steps",
            learning_rate= conf['learning_rate'],
            save_total_limit=2,
            remove_unused_columns=False,
            push_to_hub=False,
        )
        

        data_collator = DefaultDataCollator()


        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=encoded_train_dataset,
            eval_dataset=encoded_test_dataset,
            tokenizer=processor,
        )

        trainer.train()
        print("Model Trained successfully")
        return model
      


    def run(self,conf):
      updated_dataset = self.loading_dataset(conf)
      encoded_train_dataset, encoded_test_dataset, processor = self.data_preprocessing(conf,updated_dataset)
      model = self.training(conf,encoded_train_dataset, encoded_test_dataset,processor)
      # model.save_pretrained(conf["parent_dir"] + "/" + "model")
      # tokenizer = AutoTokenizer.from_pretrained(conf["model_checkpoint"])
      # # Save the tokenizer vocabulary to your Google Drive
      # tokenizer.save_pretrained(conf["parent_dir"] + "/" + "tokenizer")
      


conf = {}
# conf["data"] = new_dataset
conf["model_checkpoint"] ="microsoft/layoutlmv2-base-uncased"
conf["parent_dir"] = "/content/drive/MyDrive/Accure.ai/Document QA"
conf["epoch"]=0.2
conf["output_dir"] = "./results"
conf['batch_size'] = 4
conf['learning_rate'] = 5e-5
conf["feature_columns"] = 'question'
conf["label_column"] = 'answers'

process = document_question_answering()
process.run(conf)

